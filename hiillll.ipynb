{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12018935,"sourceType":"datasetVersion","datasetId":7561684},{"sourceId":12019761,"sourceType":"datasetVersion","datasetId":7562219}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pandas rapidfuzz unidecode sentence-transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:33:11.714868Z","iopub.execute_input":"2025-05-31T19:33:11.715318Z","iopub.status.idle":"2025-05-31T19:34:29.682037Z","shell.execute_reply.started":"2025-05-31T19:33:11.715295Z","shell.execute_reply":"2025-05-31T19:34:29.681291Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nCollecting rapidfuzz\n  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting unidecode\n  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\nDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: unidecode, rapidfuzz, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rapidfuzz-3.13.0 unidecode-1.4.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom unidecode import unidecode\nfrom rapidfuzz import fuzz\nfrom sentence_transformers import SentenceTransformer, util\n\n# Load your Excel\ndf = pd.read_excel(\"/kaggle/input/dataset/Sample_100k.xlsx\")\n\n#  Normalize Arabic \ndef normalize_arabic(text):\n    text = re.sub(r'[ًٌٍَُِّْـ]', '', text)  # Remove diacritics\n    text = re.sub(r'[إأآا]', 'ا', text)\n    text = re.sub(r'ة', 'ه', text)\n    text = re.sub(r'ى', 'ي', text)\n    return text\n\n# Universal Preprocessing \ndef preprocess(name, lang):\n    name = str(name).lower()\n    name = re.sub(r'[^\\w\\s]', '', name)  # Remove punctuation\n    name = re.sub(r'\\s+', ' ', name).strip()\n    \n    if lang == \"fr\":\n        name = unidecode(name)  # Remove accents (e.g. é → e)\n    elif lang == \"ar\":\n        name = normalize_arabic(name)\n    \n    words = name.split()\n    return ' '.join(sorted(words))\n\n# Preprocess names from both columns \narabic_names = df[\"NOM_AR\"].dropna().astype(str).tolist()\nfrench_names = df[\"NOM_FR\"].dropna().astype(str).tolist()\n\npreprocessed_ar = [preprocess(n, \"ar\") for n in arabic_names]\npreprocessed_fr = [preprocess(n, \"fr\") for n in french_names]\n\n# Prepare embeddings (optional but powerful) \nmodel = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n\nar_embeddings = model.encode(preprocessed_ar, convert_to_tensor=True)\nfr_embeddings = model.encode(preprocessed_fr, convert_to_tensor=True)\n\n#  Matching functions \n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:46:24.018671Z","iopub.execute_input":"2025-05-31T19:46:24.019272Z","iopub.status.idle":"2025-05-31T19:47:38.462135Z","shell.execute_reply.started":"2025-05-31T19:46:24.019250Z","shell.execute_reply":"2025-05-31T19:47:38.461537Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40c53b28e4d74ad1a6a6dcad31f19438"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51a72fec0fed4b6b991845435da691af"}},"metadata":{}},{"name":"stdout","text":"100.0\n Too similar to an existing name (fuzzy match).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcb7123aee0a46de8bf8f38f615a5937"}},"metadata":{}},{"name":"stdout","text":" Semantically too close to an existing name.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def is_exact_duplicate(new_name, lang):\n    clean = preprocess(new_name, lang)\n    return clean in (preprocessed_ar if lang == \"ar\" else preprocessed_fr)\n\ndef is_fuzzy_duplicate(new_name, lang, threshold=85):\n    clean = preprocess(new_name, lang)\n    names = preprocessed_ar if lang == \"ar\" else preprocessed_fr\n    for existing in names:\n        if fuzz.token_set_ratio(clean, existing) >= threshold:\n            print(existing)\n            print(fuzz.token_set_ratio(clean, existing))\n            return True\n    return False\n\ndef is_semantically_similar(new_name, lang, threshold=0.85):\n    clean = preprocess(new_name, lang)\n    vec = model.encode(clean, convert_to_tensor=True)\n    db = ar_embeddings if lang == \"ar\" else fr_embeddings\n    return util.cos_sim(vec, db).max().item() > threshold\n\n# Main Check Function \n\ndef check_name_violation(name, lang):\n    if is_exact_duplicate(name, lang):\n        return \" This name already exists (exact/reordered match).\"\n    if is_fuzzy_duplicate(name, lang):\n        return \" Too similar to an existing name (fuzzy match).\"\n    if is_semantically_similar(name, lang):\n        return \" Semantically too close to an existing name.\"\n    return \"✅ This name is acceptable.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:49:46.153405Z","iopub.execute_input":"2025-05-31T19:49:46.153687Z","iopub.status.idle":"2025-05-31T19:49:46.160706Z","shell.execute_reply.started":"2025-05-31T19:49:46.153668Z","shell.execute_reply":"2025-05-31T19:49:46.159866Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"print(check_name_violation(\"marketing center\", \"fr\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:49:49.034615Z","iopub.execute_input":"2025-05-31T19:49:49.034898Z","iopub.status.idle":"2025-05-31T19:49:49.065338Z","shell.execute_reply.started":"2025-05-31T19:49:49.034878Z","shell.execute_reply":"2025-05-31T19:49:49.064640Z"}},"outputs":[{"name":"stdout","text":"center\n100.0\n Too similar to an existing name (fuzzy match).\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom unidecode import unidecode\nfrom rapidfuzz import fuzz\nfrom sentence_transformers import SentenceTransformer, util\n\n# Load your Excel\ndf = pd.read_excel(\"/kaggle/input/dataset/Sample_100k.xlsx\")\n\n#  Normalize Arabic \ndef normalize_arabic(text):\n    text = re.sub(r'[ًٌٍَُِّْـ]', '', text)  # Remove diacritics\n    text = re.sub(r'[إأآا]', 'ا', text)\n    text = re.sub(r'ة', 'ه', text)\n    text = re.sub(r'ى', 'ي', text)\n    return text\n\n# Universal Preprocessing \ndef preprocess(name, lang):\n    name = str(name).lower()\n    name = re.sub(r'[^\\w\\s]', '', name)  # Remove punctuation\n    name = re.sub(r'\\s+', ' ', name).strip()\n    \n    if lang == \"fr\":\n        name = unidecode(name)  # Remove accents (e.g. é → e)\n    elif lang == \"ar\":\n        name = normalize_arabic(name)\n    \n    words = name.split()\n    return ' '.join(sorted(words))\n\n# Preprocess names from both columns \narabic_names = df[\"NOM_AR\"].dropna().astype(str).tolist()\nfrench_names = df[\"NOM_FR\"].dropna().astype(str).tolist()\n\npreprocessed_ar = [preprocess(n, \"ar\") for n in arabic_names]\npreprocessed_fr = [preprocess(n, \"fr\") for n in french_names]\n\n# Prepare embeddings (optional but powerful) \nmodel = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n\nar_embeddings = model.encode(preprocessed_ar, convert_to_tensor=True)\nfr_embeddings = model.encode(preprocessed_fr, convert_to_tensor=True)\n\n#  Matching functions \n\ndef check_exact_match(name, lang):\n    clean = preprocess(name, lang)\n    db = preprocessed_ar if lang == \"ar\" else preprocessed_fr\n\n    for existing in db:\n        if clean == existing:\n            return {\n                \"type\": \"exact\",\n                \"match\": existing,\n                \"score\": 100,\n                \"details\": \"Same words (reordered or identical)\"\n            }\n    return None\n\nfrom rapidfuzz import fuzz, process\n\ndef check_fuzzy_match(name, lang, threshold=85):\n    clean = preprocess(name, lang)\n    db = preprocessed_ar if lang == \"ar\" else preprocessed_fr\n\n    best_match, score, _ = process.extractOne(clean, db, scorer=fuzz.token_set_ratio)\n    \n    if score >= threshold:\n        common_words = set(clean.split()).intersection(set(best_match.split()))\n        return {\n            \"type\": \"fuzzy\",\n            \"match\": best_match,\n            \"score\": score,\n            \"details\": f\"Shared words: {', '.join(common_words)}\"\n        }\n    return None\ndef check_semantic_match(name, lang, threshold=0.85):\n    clean = preprocess(name, lang)\n    query_vec = model.encode(clean, convert_to_tensor=True)\n    db_vecs = ar_embeddings if lang == \"ar\" else fr_embeddings\n    db_names = preprocessed_ar if lang == \"ar\" else preprocessed_fr\n\n    cos_scores = util.cos_sim(query_vec, db_vecs)[0]\n    best_idx = cos_scores.argmax().item()\n    best_score = cos_scores[best_idx].item()\n\n    if best_score >= threshold:\n        return {\n            \"type\": \"semantic\",\n            \"match\": db_names[best_idx],\n            \"score\": round(best_score * 100, 2),\n            \"details\": \"High semantic similarity (meaning is close)\"\n        }\n    return None\n\n# Main Check Function \n\ndef full_name_check(name, lang):\n    checks = [\n        check_exact_match(name, lang),\n        check_fuzzy_match(name, lang),\n        check_semantic_match(name, lang)\n    ]\n    \n    for result in checks:\n        if result:\n            return result\n    \n    return {\n        \"type\": \"unique\",\n        \"match\": None,\n        \"score\": 0,\n        \"details\": \"Name is sufficiently different from all existing ones\"\n    }\nresult = full_name_check(\"تكنو المستقبل\", \"ar\")\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T20:25:25.204529Z","iopub.execute_input":"2025-05-31T20:25:25.204913Z","iopub.status.idle":"2025-05-31T20:26:39.585464Z","shell.execute_reply.started":"2025-05-31T20:25:25.204892Z","shell.execute_reply":"2025-05-31T20:26:39.584745Z"},"jupyter":{"source_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c5ddf7d09bc4c279ab4d97f2c61d938"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9fb55ab48c34676822c1d0ba0d3556c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3238edf158fc4d6e89367a404ab56ec7"}},"metadata":{}},{"name":"stdout","text":"{'type': 'fuzzy', 'match': 'المستقبل', 'score': 100.0, 'details': 'Shared words: المستقبل'}\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"\n\ndef check_exact_match(name, lang):\n    clean = preprocess(name, lang)\n    db = preprocessed_ar if lang == \"ar\" else preprocessed_fr\n\n    for existing in db:\n        if clean == existing:\n            return {\n                \"type\": \"exact\",\n                \"match\": existing,\n                \"score\": 100,\n                \"details\": \"Same words (reordered or identical)\"\n            }\n    return None\n\nfrom rapidfuzz import fuzz, process\n\ndef check_fuzzy_match(name, lang, threshold=85):\n    clean = preprocess(name, lang)\n    db = preprocessed_ar if lang == \"ar\" else preprocessed_fr\n\n    best_match, score, _ = process.extractOne(clean, db, scorer=fuzz.token_set_ratio)\n    \n    if score >= threshold:\n        common_words = set(clean.split()).intersection(set(best_match.split()))\n        return {\n            \"type\": \"fuzzy\",\n            \"match\": best_match,\n            \"score\": score,\n            \"details\": f\"Shared words: {', '.join(common_words)}\"\n        }\n    return \"no matches found\"\ndef check_semantic_match(name, lang, threshold=0.85):\n    clean = preprocess(name, lang)\n    query_vec = model.encode(clean, convert_to_tensor=True)\n    db_vecs = ar_embeddings if lang == \"ar\" else fr_embeddings\n    db_names = preprocessed_ar if lang == \"ar\" else preprocessed_fr\n\n    cos_scores = util.cos_sim(query_vec, db_vecs)[0]\n    best_idx = cos_scores.argmax().item()\n    best_score = cos_scores[best_idx].item()\n\n    if best_score >= threshold:\n        return {\n            \"type\": \"semantic\",\n            \"match\": db_names[best_idx],\n            \"score\": round(best_score * 100, 2),\n            \"details\": \"High semantic similarity (meaning is close)\"\n        }\n    return None\n\n# Main Check Function \n\ndef full_name_check(name, lang):\n    checks = [\n        check_exact_match(name, lang),\n        check_fuzzy_match(name, lang),\n        #check_semantic_match(name, lang)\n    ]\n    \n    for result in checks:\n        if result:\n            return result\n    \n    return {\n        \"type\": \"unique\",\n        \"match\": None,\n        \"score\": 0,\n        \"details\": \"Name is sufficiently different from all existing ones\"\n    }\nresult = full_name_check(\"samar de   tourisme  \", \"fr\")\nprint(result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T20:29:40.776850Z","iopub.execute_input":"2025-05-31T20:29:40.777578Z","iopub.status.idle":"2025-05-31T20:29:40.865517Z","shell.execute_reply.started":"2025-05-31T20:29:40.777555Z","shell.execute_reply":"2025-05-31T20:29:40.864895Z"}},"outputs":[{"name":"stdout","text":"{'type': 'fuzzy', 'match': 'de', 'score': 100.0, 'details': 'Shared words: de'}\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}